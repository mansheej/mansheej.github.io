[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mansheej Paul",
    "section": "",
    "text": "Hi! I am a Ph.D. student at Stanford University studying the principles and mechanisms that underlie learning in neural networks. My research focuses on:\n\nHow does data affect what information and computational abilities are learned by networks?\nHow can we engineer the data distribution to train networks to be more reliable, adaptable, and controllable.\n\nRecently, I have worked on using the data distribution and properties of loss landscapes to make training more efficient, both in terms of requiring less data and training sparser networks. I am currently excited about investigating how the data distribution affects in-context learning, memorization, federated learning and continual learning.\nI am advised by Surya Ganguli and am an intern at Meta AI. Previously, I was at the RegLab at Stanford University where I worked with partners at the Internal Revenue Service and Deparment of Labor on using machine learning to make government administrative processes more efficient. Before graduate school, I did my undergraduate at Brown University where I studied random matrix theory and its application to black hole physics.\nI have also volunteered for SF New Deal where I helped research and draft their economic impact report. Whenever I get the chance, I like to go out social dancing and am part of Cardinal West Coast Swing and the Stanford Tango Club."
  },
  {
    "objectID": "pubs.html",
    "href": "pubs.html",
    "title": "Publications",
    "section": "",
    "text": "*Equal contribution\n\n\nUnmasking the Lottery Ticket Hypothesis: What’s Encoded in a Winning Ticket’s Mask?\nMansheej Paul*, Feng Chen*, Brett W. Larsen*, Jonathan Frankle, Surya Ganguli, Gintare Karolina Dziugaite\narXiv Preprint, 2022\narXiv  Thread\n\nLottery Tickets on a Data Diet: Finding Initializations with Sparse Trainable Networks\nMansheej Paul*, Brett W. Larsen*, Surya Ganguli, Jonathan Frankle, Gintare Karolina Dziugaite\nAccepted at Neural Information Processing Systems (NeurIPS), 2022\narXiv  GitHub  Thread\n\n\n\n\nDeep Learning on a Data Diet: Finding Important Examples Early in Training\nMansheej Paul, Surya Ganguli, Gintare Karolina Dziugaite\nAdvances in Neural Information Processing Systems 34 (NeurIPS), 2021\narXiv NeurIPS 2021  GitHub  Thread\n\n\n\n\nDeep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the Neural Tangent Kernel\nStanislav Fort*, Gintare Karolina Dziugaite*, Mansheej Paul, Sepideh Kharaghani, Daniel M. Roy, Surya Ganguli\nAdvances in Neural Information Processing Systems 33 (NeurIPS 2020)\narXiv NeurIPS 2020  Thread"
  },
  {
    "objectID": "pubs.html#peer-reviewed-workshops",
    "href": "pubs.html#peer-reviewed-workshops",
    "title": "Publications",
    "section": "Peer-Reviewed Workshops",
    "text": "Peer-Reviewed Workshops\nUnmasking the Lottery Ticket Hypothesis: Efficient Adaptive Pruning for Finding Winning Tickets\nMansheej Paul*, Feng Chen*, Brett W. Larsen*, Jonathan Frankle, Surya Ganguli, Gintare Karolina Dziugaite\nPoster at Has it Trained Yet? A Workshop for Algorithmic Efficiency in Practical Neural Network Training, Conference on Neural Information Processing Systems (NeurIPS), 2022\n\nPre-Training on a Data Diet: Identifying Sufficient Examples for Early Training\nMansheej Paul*, Brett W. Larsen*, Surya Ganguli, Jonathan Frankle, Gintare Karolina Dziugaite\nPoster at The First Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward, International Conference on Machine Learning (ICML), 2022\n\nLottery Tickets on a Data Diet: Identifying Sufficient Data for Finding Sparse Trainable Networks\nMansheej Paul*, Brett W. Larsen*, Surya Ganguli, Jonathan Frankle, Gintare Karolina Dziugaite  Spotlight at Sparsity in Neural Networks Workshop (SNN), 2022"
  },
  {
    "objectID": "pubs.html#talks",
    "href": "pubs.html#talks",
    "title": "Publications",
    "section": "Talks",
    "text": "Talks\n\nGoogle Brain, November 2022\nMosaicML, October 2022"
  }
]