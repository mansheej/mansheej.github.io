---
title: "Mansheej Paul"
image: _files/profile.jpg
about:
    template: trestles
    image-width: 12em
    image-shape: round
    links:
        - icon: envelope
          text: Email
          href: mailto:mansheejp@gmail.com
        - icon: twitter
          text: Twitter
          href: https://twitter.com/mansiege
        - text: "{{< ai google-scholar >}} Google Scholar"
          href: https://scholar.google.co.uk/citations?hl=en&user=gmZt3VgAAAAJ
        
---

Hi, I am a research scientist at Mosaic AI Research, Databricks. 

My research spans pre-training and post-training LLMs with a focus on optimizing data quality, distribution and curricula. I currently build synthetic data pipelines that scale inference compute to create diverse generations and develop strategies to verify and filter them into high-quality training data. Through rigorous evaluation of model behavior and how it is shaped by training data properties, I aim to create reliable, consistent and trustworthy AI systems.

Previously, I did my Ph.D. in Applied Physics at Stanford University where I was advised by [Surya Ganguli](https://ganguli-gang.stanford.edu/surya.html) and spent some time as a research intern at [FAIR, Meta AI](https://ai.facebook.com/). During this time, I worked on the science of deep learning through the lens of data, loss landscapes and neural tangent kernels.

You can find my publications on [Google Scholar](https://scholar.google.co.uk/citations?hl=en&user=gmZt3VgAAAAJ).

<!-- _Hi!_ I am a Ph.D. student at Stanford University studying the principles and mechanisms that underlie learning in neural networks. My research focuses on:

- How does data affect what information and computational abilities are learned by networks?
- How can we engineer the data distribution to train networks to be more reliable, adaptable, and controllable?

Recently, I have worked on using the data distribution and loss landscape properties to make training more efficient, both in terms of requiring less data and training sparser networks. I am currently excited about investigating how the data distribution influences in-context learning, memorization, federated learning and continual learning.

I am advised by [Surya Ganguli](https://ganguli-gang.stanford.edu/surya.html) and am an intern at [Meta AI](https://ai.facebook.com/). Previously, I was at the [RegLab](https://reglab.stanford.edu/) at Stanford University where I worked with partners at the Internal Revenue Service and Department of Labor on using machine learning to make government administrative processes more efficient. Before graduate school, I did my undergraduate at Brown University where I studied random matrix theory and its application to black hole physics. -->

I have also volunteered for [SF New Deal](https://sfnewdeal.org/) where I helped research and draft their [economic impact report](https://static1.squarespace.com/static/5e75a094a5e7414c1d12b773/t/5f8ddd68286841680af0c5ef/1603132818535/impact-report). Whenever I get the chance, I like to go out social dancing, mostly West Coast Swing.
<!-- and am part of [Cardinal West Coast Swing](https://www.facebook.com/cardinalswing/) and the [Stanford Tango Club](https://www.facebook.com/groups/StanfordTangoClub/?mibextid=6NoCDW). -->